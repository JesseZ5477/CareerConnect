{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86XWvjxeZV8I"
   },
   "source": [
    "# LLM-Native Resume Matching Solution\n",
    "\n",
    "This notebook demonstrates the implementation of an LLM-native resume matching solution that transforms traditional resume screening into an AI-powered, conversational experience. This aims to streamline the recruitment process by automating candidate matching and providing natural language interaction for recruiters.\n",
    "\n",
    "## An LLM-native approach that uses generative AI to:\n",
    "  - Extract structured information from resumes automatically\n",
    "  - Enable natural language queries for candidate search\n",
    "  - Provide matching between job descriptions and candidates\n",
    "  - Offer detailed analysis of why candidates match specific roles\n",
    "\n",
    "## Workflow\n",
    "1. **Data Processing**\n",
    "   - Parse PDF resumes using LlamaParse\n",
    "   - Extract structured metadata (skills, education, domain) using LLMs\n",
    "   - Store processed documents in LlamaCloud for efficient retrieval\n",
    "\n",
    "2. **Index Creation**\n",
    "   - Create a Pipeline/ Index using LlamaCloud\n",
    "   - Configure embedding and transformation settings\n",
    "   - Upload processed documents with metadata\n",
    "\n",
    "3. **Query Processing**\n",
    "   - Support two types of queries:\n",
    "     - Natural language queries from recruiters (e.g., \"Find Java developers from US universities\")\n",
    "     - Job description-based matching\n",
    "   - Extract relevant metadata filters from queries using LLMs\n",
    "   - Retrieve matching candidates based on metadata and semantic search\n",
    "\n",
    "4. **Candidate Analysis**\n",
    "   - Generate detailed analysis of why candidates match job requirements\n",
    "   - Compare candidate qualifications against job criteria\n",
    "   - Provide insights into strengths and potential gaps.\n",
    "\n",
    "\n",
    "**NOTE**: Sample dataset consisting of 30 resumes (10 each from Information Technology, Sales, and Finance domains) sourced from the [Kaggle Resume Dataset](https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset). This smaller dataset allows for easier experimentation and clearer demonstration of the concept.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqSD3115Uoep"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install `llama-index`, `llama-index-indices-managed-llama-cloud`, `llama-parse` and `llama-cloud`. \n",
    "\n",
    "These packages are tools for building, parsing, and managing LLM applications on LlamaIndex's cloud platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tbgPfxqILy5e",
    "outputId": "a28f1b7e-1ce5-45b5-c75e-080d15b70238"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (0.12.10)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (0.6.3)\n",
      "Requirement already satisfied: llama-parse in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (0.5.19)\n",
      "Requirement already satisfied: llama-cloud in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (0.1.8)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index) (0.4.1)\n",
      "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.10 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index) (0.12.10.post1)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index) (0.3.13)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index) (0.4.2)\n",
      "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index) (0.4.2)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-parse) (8.1.8)\n",
      "Requirement already satisfied: pydantic!=2.10 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-parse) (2.10.5)\n",
      "Requirement already satisfied: certifi<2025.0.0,>=2024.7.4 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-cloud) (2024.12.14)\n",
      "Requirement already satisfied: httpx>=0.20.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-cloud) (0.28.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from click<9.0.0,>=8.1.7->llama-parse) (0.4.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from httpx>=0.20.0->llama-cloud) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from httpx>=0.20.0->llama-cloud) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from httpx>=0.20.0->llama-cloud) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from httpcore==1.*->httpx>=0.20.0->llama-cloud) (0.14.0)\n",
      "Requirement already satisfied: openai>=1.14.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.59.6)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.10->llama-index) (2.0.37)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (3.11.11)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (1.2.15)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (2024.12.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (3.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (2.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (11.1.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (1.17.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.3)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.1.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: joblib in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from pydantic!=2.10->llama-parse) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from pydantic!=2.10->llama-parse) (2.27.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama-index) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama-index) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama-index) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama-index) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama-index) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama-index) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama-index) (1.18.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.10->llama-index) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.10->llama-index) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.10->llama-index) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.10->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.10->llama-index) (3.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2024.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.10->llama-index) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\runqz\\anaconda3\\envs\\llm\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U llama-index llama-index-indices-managed-llama-cloud llama-parse llama-cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yr0EAvWqMCx5"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abREdlc9Us-5"
   },
   "source": [
    "## Setup API Keys\n",
    "\n",
    "Utilize `gpt-4o-mini` from OpenAI's LLM and LlamaCloud, an enterprise platform designed for building LLM applications.\n",
    "\n",
    "Set up the `OPENAI_API_KEY` and `LLAMA_CLOUD_API_KEY` as they are paid features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tG3WZeDYMF8h"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\" # Get your API key from https://platform.openai.com/account/api-keys\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-ulskyur4sbyH90bJrcJSAzP4BdY6WqtxljOL4zPiXR2WvuVi\" # Get your API key from https://cloud.llamaindex.ai/api-key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeVZhD86UvXc"
   },
   "source": [
    "## Setup LLM\n",
    "\n",
    "We will initialize `gpt-4o-mini` OpenAI LLM. We could also use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oI-rcTFWMHTk"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample jd and cv source\n",
    "\n",
    "Sampled data from [Kaggle Resume Dataset](https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset) and `job_description.pdf`.\n",
    "\n",
    "`sampled_dataset` - contains 10 each from Information Technology, Sales, and Finance domains.\n",
    "\n",
    "`job_description.pdf` - This is the job description file we will use to retrieve candidate profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the sampled data\n",
    "!wget --content-disposition \"https://www.dropbox.com/scl/fo/v1mn1rxqz2ifqtx009owh/APHC7xPTQ7BiRZv0BKZ7cag?rlkey=rh09o73172vzifjqlsmw4fhmo&st=v220giff&dl=1\"\n",
    "\n",
    "# make a directory to store the data\n",
    "!mkdir -p \"./sampled_data\"\n",
    "\n",
    "# unzip the data\n",
    "!unzip sampled_data.zip -d \"./sampled_data\"\n",
    "\n",
    "# Download the job description file\n",
    "!wget -O job_description.pdf \"https://www.dropbox.com/scl/fi/b1djiczj6vy8s6h4isvmr/job_description.pdf?rlkey=drpkd2exj8edkuw1f0evhvqfx&st=2i2wb801&dl=1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHwuhLUBUxkj"
   },
   "source": [
    "## Utils\n",
    "\n",
    "Define some functions for further processing before loading into LLM.\n",
    "\n",
    "1. `parse_files`: Processes PDF files using LlamaParse and converts them to markdown format with updated metadata\n",
    "\n",
    "2. `list_pdf_files`: Recursively finds all PDF files in a directory and its subdirectories\n",
    "\n",
    "3. `Metadata`: Pydantic model to structure resume metadata including domain, skills, and educational country information.\n",
    "\n",
    "4. `create_llamacloud_pipeline`: Creates or updates a LlamaCloud pipeline with specified configurations.\n",
    "\n",
    "5. `get_metadata`: Extracts structured metadata from resume text using an LLM.\n",
    "\n",
    "6. `get_document_upload`: Prepares a document for cloud upload by combining text and extracted metadata.\n",
    "\n",
    "7. `upload_documents`: Batch uploads documents to LlamaCloud pipeline with parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6-pM24SBMNse"
   },
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "from pathlib import Path\n",
    "from llama_index.core import Document\n",
    "from llama_cloud.types import CloudDocumentCreate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from llama_cloud.client import LlamaCloud\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.async_utils import run_jobs\n",
    "\n",
    "def parse_files(pdf_files):\n",
    "    \"\"\"Function to parse the pdf files using LlamaParse in markdown format\"\"\"\n",
    "\n",
    "    parser = LlamaParse(\n",
    "        result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n",
    "        num_workers=4,  # if multiple files passed, split in `num_workers` API calls\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for index, pdf_file in enumerate(pdf_files):\n",
    "        print(f\"Processing file {index + 1}/{len(pdf_files)}: {pdf_file}\")\n",
    "        # Parsing and chunking pdf\n",
    "        docs = parser.load_data(pdf_file)\n",
    "        # Updating metadata with filepath\n",
    "        for doc in docs:\n",
    "          doc.metadata.update({'filepath': pdf_file})\n",
    "        documents.append(docs)\n",
    "\n",
    "    return documents\n",
    "\n",
    "def list_pdf_files(directory):\n",
    "    # List all .pdf files recursively using pathlib\n",
    "    # rglob ('recursive glob') searches through all subdirectories\n",
    "    pdf_files = [str(file) for file in Path(directory).rglob('*.pdf')]\n",
    "    return pdf_files\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    \"\"\"\n",
    "    A data model representing key professional and educational metadata extracted from a resume.\n",
    "    This class captures essential candidate information including technical/professional skills\n",
    "    and the geographical distribution of their educational background.\n",
    "\n",
    "    Attributes:\n",
    "        skills (List[str]): Technical and professional competencies of the candidate\n",
    "        country (List[str]): Countries where the candidate pursued formal education\n",
    "\n",
    "    Example:\n",
    "        {\n",
    "            \"skills\": [\"Python\", \"Machine Learning\", \"SQL\", \"Project Management\"],\n",
    "            \"country\": [\"United States\", \"India\"],\n",
    "            \"domain\": \"Information Technology\"\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    domain: str = Field(...,\n",
    "                        description=\"The domain of the candidate can be one of SALES/ IT/ FINANCE\"\n",
    "                                    \"Returns an empty string if no domain is identified.\")\n",
    "\n",
    "    skills: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"List of technical, professional, and soft skills extracted from the resume. \"\n",
    "                   \"and domain expertise. Returns an empty list if no skills are identified.\"\n",
    "    )\n",
    "\n",
    "    country: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"List of countries where the candidate completed their formal education, Only extract the country.\"\n",
    "                   \"Returns an empty list if countries are not specified.\"\n",
    "    )\n",
    "\n",
    "def create_llamacloud_pipeline(pipeline_name, embedding_config, transform_config, data_sink_id=None):\n",
    "    \"\"\"Function to create a pipeline in llamacloud\"\"\"\n",
    "\n",
    "    client = LlamaCloud(token=os.environ[\"LLAMA_CLOUD_API_KEY\"])\n",
    "\n",
    "    pipeline = {\n",
    "        'name': pipeline_name,\n",
    "        'transform_config': transform_config,\n",
    "        'embedding_config': embedding_config,\n",
    "        'data_sink_id': data_sink_id\n",
    "    }\n",
    "\n",
    "    pipeline = client.pipelines.upsert_pipeline(request=pipeline)\n",
    "\n",
    "    return client, pipeline\n",
    "\n",
    "async def get_metadata(text):\n",
    "    \"\"\"Function to get the metadata from the given resume of the candidate\"\"\"\n",
    "    prompt_template = PromptTemplate(\"\"\"Generate skills, and country of the education for the given candidate resume.\n",
    "\n",
    "    Resume of the candidate:\n",
    "\n",
    "    {text}\"\"\")\n",
    "\n",
    "    metadata = await llm.astructured_predict(\n",
    "        Metadata,\n",
    "        prompt_template,\n",
    "        text=text,\n",
    "    )\n",
    "\n",
    "    return metadata\n",
    "\n",
    "async def get_document_upload(documents, llm):\n",
    "    \"\"\"For every resume, feed its chuncks to LLM, get the metadata and save/update to vector store\"\"\"\n",
    "    full_text = \"\\n\\n\".join([doc.text for doc in documents])\n",
    "\n",
    "    # Get the file path of the resume\n",
    "    file_path = documents[0].metadata['filepath']\n",
    "\n",
    "    # Extract metadata from the resume\n",
    "    extracted_metadata = await get_metadata(full_text)\n",
    "\n",
    "    skills = list(set(getattr(extracted_metadata, 'skills', [])))\n",
    "    country = list(set(getattr(extracted_metadata, 'country', [])))\n",
    "    domain = getattr(extracted_metadata, 'domain', '')\n",
    "\n",
    "    # Not mandatory to maintain the Global sets\n",
    "    global_skills.extend(skills)\n",
    "    global_countries.extend(country)\n",
    "    global_domains.append(domain)\n",
    "\n",
    "    return CloudDocumentCreate(\n",
    "                text=full_text,\n",
    "                metadata={\n",
    "                    'skills': skills,\n",
    "                    'country': country,\n",
    "                    'domain': domain,\n",
    "                    'file_path': file_path\n",
    "                }\n",
    "            )\n",
    "\n",
    "async def upload_documents(client, pipeline, documents):\n",
    "    \"\"\"Function to upload the documents to the cloud\"\"\"\n",
    "\n",
    "    # Upload the documents to the cloud\n",
    "    extract_jobs = []\n",
    "    for doc in documents:\n",
    "        extract_jobs.append(get_document_upload(doc, llm))\n",
    "\n",
    "    documents_upload_objs = await run_jobs(extract_jobs, workers=4)\n",
    "\n",
    "    _ = client.pipelines.create_batch_pipeline_documents(pipeline.id, request=documents_upload_objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSaUjbxuVrbI"
   },
   "source": [
    "## Parse the files\n",
    "\n",
    "Parse all the sampled resumes using `LlamaParse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zQsRIXOkMZKf",
    "outputId": "a8419c5e-25b4-45fc-ecd5-19d7e73b06a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1/30: sampled_data\\FINANCE\\12071138.pdf\n",
      "Started parsing the file under job_id a8241a7f-d168-4fc0-9a67-fd45cecf17d8\n",
      "Processing file 2/30: sampled_data\\FINANCE\\14408510.pdf\n",
      "Started parsing the file under job_id 60c620ab-8854-4073-94e9-f451166d57df\n",
      "Processing file 3/30: sampled_data\\FINANCE\\15891494.pdf\n",
      "Started parsing the file under job_id 91df3704-9234-438c-9bee-00749e3270fb\n",
      "Processing file 4/30: sampled_data\\FINANCE\\19540089.pdf\n",
      "Started parsing the file under job_id 8c735bfc-3ae9-45c3-9bc4-d36e02133feb\n",
      ".Processing file 5/30: sampled_data\\FINANCE\\24967652.pdf\n",
      "Started parsing the file under job_id c55c8685-81b4-4776-a97e-b4e20fb666a5\n",
      "Processing file 6/30: sampled_data\\FINANCE\\25101183.pdf\n",
      "Started parsing the file under job_id 25a47859-7597-4634-8a08-0eecb6163cfd\n",
      "Processing file 7/30: sampled_data\\FINANCE\\26961846.pdf\n",
      "Started parsing the file under job_id f04110d3-92cc-4872-a8c9-96e080534d31\n",
      "Processing file 8/30: sampled_data\\FINANCE\\28398216.pdf\n",
      "Started parsing the file under job_id b5d4d38b-e7ce-4d49-a46f-01219e4dcefe\n",
      "Processing file 9/30: sampled_data\\FINANCE\\38907798.pdf\n",
      "Started parsing the file under job_id ef41129c-cee2-408c-bca0-a05d68199ada\n",
      "Processing file 10/30: sampled_data\\FINANCE\\84373843.pdf\n",
      "Started parsing the file under job_id e8b3b7da-e62a-435e-b9f2-08b12ee89b44\n",
      "Processing file 11/30: sampled_data\\IT\\16899268.pdf\n",
      "Started parsing the file under job_id 9025aaf6-240c-49b3-be55-e2cf447cebec\n",
      "Processing file 12/30: sampled_data\\IT\\17641670.pdf\n",
      "Started parsing the file under job_id 0e4b958e-7ee4-4918-ba78-7d1786512eb0\n",
      "Processing file 13/30: sampled_data\\IT\\18159866.pdf\n",
      "Started parsing the file under job_id 7f6444a0-5ed1-4630-a340-515605f4606d\n",
      "Processing file 14/30: sampled_data\\IT\\23527321.pdf\n",
      "Started parsing the file under job_id 226c9c47-7a65-4cc1-aeef-797c7ee275ca\n",
      "Processing file 15/30: sampled_data\\IT\\25959103.pdf\n",
      "Started parsing the file under job_id 5e3a3657-8e40-464d-9e61-3f4866b789ea\n",
      "Processing file 16/30: sampled_data\\IT\\27536013.pdf\n",
      "Started parsing the file under job_id 63efd6f1-3f7e-4bde-8859-ce62b7893aaf\n",
      "Processing file 17/30: sampled_data\\IT\\38753827.pdf\n",
      "Started parsing the file under job_id 4352e885-3799-4e54-8fc5-ad76b7c2daf3\n",
      "Processing file 18/30: sampled_data\\IT\\52246737.pdf\n",
      "Started parsing the file under job_id 9bb3a237-4b7b-4ba5-8444-2c520a31bf03\n",
      "Processing file 19/30: sampled_data\\IT\\52618188.pdf\n",
      "Started parsing the file under job_id b67ee2cc-7e58-46d6-85ed-c4f8235a6e49\n",
      "Processing file 20/30: sampled_data\\IT\\57002858.pdf\n",
      "Started parsing the file under job_id a65acb8e-bf94-47a3-a96f-f5a315ea52bf\n",
      "Processing file 21/30: sampled_data\\SALES\\12696104.pdf\n",
      "Started parsing the file under job_id 72020784-bf87-461a-b8ca-1ac8635aa295\n",
      "Processing file 22/30: sampled_data\\SALES\\17509935.pdf\n",
      "Started parsing the file under job_id f27bfcfd-1f38-487e-92d3-a3ed55283c41\n",
      "Processing file 23/30: sampled_data\\SALES\\17704246.pdf\n",
      "Started parsing the file under job_id a6204b58-5e5f-4441-8603-a9e5c99de4f8\n",
      "Processing file 24/30: sampled_data\\SALES\\19473948.pdf\n",
      "Started parsing the file under job_id a0aa31ec-56ac-47f7-8da7-5751da597296\n",
      "Processing file 25/30: sampled_data\\SALES\\25315791.pdf\n",
      "Started parsing the file under job_id 95ef73c6-7314-42b4-906b-0e5f90bbef2e\n",
      "Processing file 26/30: sampled_data\\SALES\\28198029.pdf\n",
      "Started parsing the file under job_id cedeb2b7-329f-4a4f-b021-1427a22e0ab7\n",
      "Processing file 27/30: sampled_data\\SALES\\30608780.pdf\n",
      "Started parsing the file under job_id 8ee5ebe5-4ad0-4c33-bccd-319f2a2dc529\n",
      "Processing file 28/30: sampled_data\\SALES\\31199035.pdf\n",
      "Started parsing the file under job_id 57678167-b981-4fc8-b996-250aea2a2a89\n",
      "Processing file 29/30: sampled_data\\SALES\\33236701.pdf\n",
      "Started parsing the file under job_id 5d70f2ba-57f9-4af1-9c2e-1f0060612223\n",
      "Processing file 30/30: sampled_data\\SALES\\55097118.pdf\n",
      "Started parsing the file under job_id f5568261-6b1f-4a5b-970b-68c2b8347db0\n"
     ]
    }
   ],
   "source": [
    "directory = './sampled_data/'\n",
    "pdf_files = list_pdf_files(directory)\n",
    "\n",
    "documents = parse_files(pdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_gtZk0ZVtrS"
   },
   "source": [
    "## (Not Mandatory) keep a track of skills, countries and domains.\n",
    "\n",
    "We will track `skills`, `countries`, and `domains` in each parsed resume.\n",
    "\n",
    "Here, we will initialize lists for `global_skills`, `global_countries`, and `global_domains` to monitor these attributes.\n",
    "\n",
    "Global attributes make the metadata filter more strict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jvdY7FLeMbL9"
   },
   "outputs": [],
   "source": [
    "global_skills = []\n",
    "global_countries = []\n",
    "global_domains = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYzBr6tsWVoI"
   },
   "source": [
    "## Create LlamaCloud Pipeline/ Index\n",
    "\n",
    "Define `embedding_config` and `transform_config` to set the `OPENAI_EMBEDDING`, `chunk_size`, and `chunk_overlap` parameters needed for creating an index on `LlamaCloud`. We can tune the embedding model if needed.\n",
    "\n",
    "Then create a pipeline/index on `LlamaCloud` under the name `resume_matching`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cLh_E7AyMdsq"
   },
   "outputs": [
    {
     "ename": "ApiError",
     "evalue": "status_code: 400, body: {'detail': 'We encountered an error while validating the embedding connection. Please check the credentials and try again.'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mApiError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 19\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Transformation auto config\u001b[39;00m\n\u001b[0;32m     11\u001b[0m transform_config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     }\n\u001b[0;32m     17\u001b[0m }\n\u001b[1;32m---> 19\u001b[0m client, pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_llamacloud_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresume_matching\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 85\u001b[0m, in \u001b[0;36mcreate_llamacloud_pipeline\u001b[1;34m(pipeline_name, embedding_config, transform_config, data_sink_id)\u001b[0m\n\u001b[0;32m     76\u001b[0m client \u001b[38;5;241m=\u001b[39m LlamaCloud(token\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLAMA_CLOUD_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     78\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: pipeline_name,\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransform_config\u001b[39m\u001b[38;5;124m'\u001b[39m: transform_config,\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding_config\u001b[39m\u001b[38;5;124m'\u001b[39m: embedding_config,\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_sink_id\u001b[39m\u001b[38;5;124m'\u001b[39m: data_sink_id\n\u001b[0;32m     83\u001b[0m }\n\u001b[1;32m---> 85\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipelines\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsert_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m client, pipeline\n",
      "File \u001b[1;32mc:\\Users\\runqz\\anaconda3\\envs\\LLM\\Lib\\site-packages\\llama_cloud\\resources\\pipelines\\client.py:187\u001b[0m, in \u001b[0;36mPipelinesClient.upsert_pipeline\u001b[1;34m(self, project_id, organization_id, request)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ApiError(status_code\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mstatus_code, body\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ApiError(status_code\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mstatus_code, body\u001b[38;5;241m=\u001b[39m_response_json)\n",
      "\u001b[1;31mApiError\u001b[0m: status_code: 400, body: {'detail': 'We encountered an error while validating the embedding connection. Please check the credentials and try again.'}"
     ]
    }
   ],
   "source": [
    "# Embedding config\n",
    "embedding_config = {\n",
    "    'type': 'OPENAI_EMBEDDING',\n",
    "    'component': {\n",
    "        'api_key': os.environ[\"OPENAI_API_KEY\"], # editable\n",
    "        'model_name': 'text-embedding-ada-002' # editable\n",
    "    }\n",
    "}\n",
    "\n",
    "# Transformation auto config\n",
    "transform_config = {\n",
    "    'mode': 'auto',\n",
    "    'config': {\n",
    "        'chunk_size': 1024,\n",
    "        'chunk_overlap': 20\n",
    "    }\n",
    "}\n",
    "\n",
    "client, pipeline = create_llamacloud_pipeline('resume_matching', embedding_config, transform_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTNqPyNZWYuC"
   },
   "source": [
    "## Upload Documents\n",
    "\n",
    "Once the index/pipeline is created, we will upload all the parsed resumes (documents) using the `upload_documents` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MpHcng41MfS4"
   },
   "outputs": [],
   "source": [
    "await upload_documents(client, pipeline, documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efO7c5IHWa4S"
   },
   "source": [
    "## Connect to LlamaCloud Index\n",
    "\n",
    "Here, we connect to the `resume_matching` index that was created on `LlamaCloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2AVm31UbMgnZ"
   },
   "outputs": [],
   "source": [
    "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
    "\n",
    "index = LlamaCloudIndex(\n",
    "  name=\"resume_matching\",\n",
    "  project_name=\"Default\",\n",
    "  organization_id=\"e9677bd6-6b46-4f11-9348-b2ccb3e88d42\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "An5NXm0mXBql"
   },
   "source": [
    "## Utils for Candidate retrieval.\n",
    "\n",
    "Once the index is created, we need to retrieve candidate profiles based on HR queries. Here, we will define some functions for this purpose.\n",
    "\n",
    "1. `get_query_metadata`: Extracts structured metadata from user queries by matching against existing global metadata\n",
    "\n",
    "2. `candidates_retriever_from_query`: Retrieves relevant candidate profiles based on user query using metadata filters\n",
    "\n",
    "3. `get_candidates_file_paths`: Extracts unique file paths from retrieved candidate metadata\n",
    "\n",
    "4. `candidates_retriever_from_jd`: Retrieves matching candidate profiles based on job description using metadata filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "idSIbIS_MiDC"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import (\n",
    "    MetadataFilter,\n",
    "    MetadataFilters,\n",
    "    FilterOperator,\n",
    "    FilterCondition\n",
    ")\n",
    "async def get_query_metadata(text):\n",
    "    \"\"\"Function to get the metadata from the given user query\"\"\"\n",
    "    prompt_template = PromptTemplate(\"\"\"Generate skills, and country of the education for the given user query.\n",
    "\n",
    "    Extracted metadata should be from the following items:\n",
    "\n",
    "    skills: {global_skills}\n",
    "    countries: {global_countries}\n",
    "    domains: {global_domains}\n",
    "    user query:\n",
    "\n",
    "    {text}\"\"\")\n",
    "\n",
    "    extracted_metadata = await llm.astructured_predict(\n",
    "        Metadata,\n",
    "        prompt_template,\n",
    "        text=text,\n",
    "        global_skills=global_skills,\n",
    "        global_countries=global_countries,\n",
    "        global_domains=global_domains\n",
    "    )\n",
    "\n",
    "    return extracted_metadata\n",
    "\n",
    "async def candidates_retriever_from_query(query: str):\n",
    "    \"\"\"Synthesizes an answer to your question by feeding in an entire relevant document as context.\"\"\"\n",
    "    print(f\"> User query string: {query}\")\n",
    "    # Use structured predict to infer the metadata filters and query string.\n",
    "    metadata_info = await get_query_metadata(query)\n",
    "    filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(key=\"domain\", operator=FilterOperator.EQ, value=metadata_info.domain),\n",
    "        MetadataFilter(key=\"country\", operator=FilterOperator.IN, value=metadata_info.country),\n",
    "        MetadataFilter(key=\"skills\", operator=FilterOperator.IN, value=metadata_info.skills)\n",
    "    ],\n",
    "    condition=FilterCondition.OR\n",
    ")\n",
    "    print(f\"> Inferred filters: {filters.json()}\")\n",
    "    retriever = index.as_retriever(\n",
    "    retrieval_mode=\"chunks\",\n",
    "    metadata_filters=filters,\n",
    "    )\n",
    "    # run query\n",
    "    return retriever.retrieve(query)\n",
    "\n",
    "def get_candidates_file_paths(candidates):\n",
    "\n",
    "  file_paths = []\n",
    "  for candidate in candidates:\n",
    "    file_paths.append(candidate.metadata['file_path'])\n",
    "\n",
    "  return list(set(file_paths))\n",
    "\n",
    "async def candidates_retriever_from_jd(job_description: str):\n",
    "    # Use structured predict to infer the metadata filters and query string.\n",
    "    metadata_info = await get_metadata(job_description)\n",
    "    filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(key=\"domain\", operator=FilterOperator.EQ, value=metadata_info.domain),\n",
    "        MetadataFilter(key=\"country\", operator=FilterOperator.IN, value=metadata_info.country),\n",
    "        MetadataFilter(key=\"skills\", operator=FilterOperator.IN, value=metadata_info.skills)\n",
    "    ],\n",
    "    condition=FilterCondition.OR\n",
    ")\n",
    "    print(f\"> Inferred filters: {filters.json()}\")\n",
    "    retriever = index.as_retriever(\n",
    "    retrieval_mode=\"chunks\",\n",
    "    metadata_filters=filters,\n",
    "    )\n",
    "    # run query\n",
    "    return retriever.retrieve(job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0LGRz-MXRtl"
   },
   "source": [
    "## Retrieve based on HR query\n",
    "\n",
    "Let's test the process based on a usual sample HR query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MkMluwweMj7O",
    "outputId": "4648f0e3-37f5-47ba-e096-d2e02a4977d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> User query string: I want someone who studied in US, Java developer, and worked in IT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\runqz\\AppData\\Local\\Temp\\ipykernel_25868\\246358514.py:44: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  print(f\"> Inferred filters: {filters.json()}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Inferred filters: {\"filters\":[{\"key\":\"domain\",\"value\":\"IT\",\"operator\":\"==\"},{\"key\":\"country\",\"value\":[\"United States\"],\"operator\":\"in\"},{\"key\":\"skills\",\"value\":[\"Java developer\"],\"operator\":\"in\"}],\"condition\":\"or\"}\n"
     ]
    }
   ],
   "source": [
    "query = \"I want someone who studied in US, Java developer, and worked in IT\"\n",
    "nodes = await candidates_retriever_from_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='f2f2b465-4976-4137-9ec3-46e7e9914051', embedding=None, metadata={'skills': ['Process improvement', 'Program management', 'Team player', 'Data analysis', 'Effective multi-tasker', 'Results-oriented', 'Teambuilding', 'Strategic planning', 'Extensive technology experience', 'Requirements gathering', 'Project management', 'Team leader'], 'country': ['United States'], 'domain': 'IT', 'file_path': 'sampled_data\\\\IT\\\\16899268.pdf', 'pipeline_id': '14ce0924-a864-4147-8a64-f1775b8cdb2d', 'header_path': '/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8178ca53-6244-4d3b-b828-e38acea5159d', node_type='4', metadata={'skills': ['Process improvement', 'Program management', 'Team player', 'Data analysis', 'Effective multi-tasker', 'Results-oriented', 'Teambuilding', 'Strategic planning', 'Extensive technology experience', 'Requirements gathering', 'Project management', 'Team leader'], 'country': ['United States'], 'domain': 'IT', 'file_path': 'sampled_data\\\\IT\\\\16899268.pdf', 'pipeline_id': '14ce0924-a864-4147-8a64-f1775b8cdb2d'}, hash='2a681f08b9e55605f86e72fa95440533ca48fe87b8be2d6821f7571b9932d663'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='0e49536c-9a18-4e88-bab9-b3fc3cd979b1', node_type='1', metadata={'skills': ['Process improvement', 'Program management', 'Team player', 'Data analysis', 'Effective multi-tasker', 'Results-oriented', 'Teambuilding', 'Strategic planning', 'Extensive technology experience', 'Requirements gathering', 'Project management', 'Team leader'], 'country': ['United States'], 'domain': 'IT', 'file_path': 'sampled_data\\\\IT\\\\16899268.pdf', 'pipeline_id': '14ce0924-a864-4147-8a64-f1775b8cdb2d', 'header_path': '/'}, hash='e392bfa0de58d2fc6b4c65bac5399d12761d4257d5fcb9361c8c14fa354fe99d')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='# Work History', mimetype='text/plain', start_char_idx=0, end_char_idx=14, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.34169233),\n",
       " NodeWithScore(node=TextNode(id_='967fe9c0-2e07-4f74-958d-f16765d4aa0b', embedding=None, metadata={'skills': ['Process improvement', 'Program management', 'Team player', 'Data analysis', 'Effective multi-tasker', 'Results-oriented', 'Teambuilding', 'Strategic planning', 'Extensive technology experience', 'Requirements gathering', 'Project management', 'Team leader'], 'country': ['United States'], 'domain': 'IT', 'file_path': 'sampled_data\\\\IT\\\\16899268.pdf', 'pipeline_id': '14ce0924-a864-4147-8a64-f1775b8cdb2d', 'header_path': '/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8178ca53-6244-4d3b-b828-e38acea5159d', node_type='4', metadata={'skills': ['Process improvement', 'Program management', 'Team player', 'Data analysis', 'Effective multi-tasker', 'Results-oriented', 'Teambuilding', 'Strategic planning', 'Extensive technology experience', 'Requirements gathering', 'Project management', 'Team leader'], 'country': ['United States'], 'domain': 'IT', 'file_path': 'sampled_data\\\\IT\\\\16899268.pdf', 'pipeline_id': '14ce0924-a864-4147-8a64-f1775b8cdb2d'}, hash='2a681f08b9e55605f86e72fa95440533ca48fe87b8be2d6821f7571b9932d663'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='45f753f2-9b8f-4df5-bcce-62d4eef349e7', node_type='1', metadata={'skills': ['Process improvement', 'Program management', 'Team player', 'Data analysis', 'Effective multi-tasker', 'Results-oriented', 'Teambuilding', 'Strategic planning', 'Extensive technology experience', 'Requirements gathering', 'Project management', 'Team leader'], 'country': ['United States'], 'domain': 'IT', 'file_path': 'sampled_data\\\\IT\\\\16899268.pdf', 'pipeline_id': '14ce0924-a864-4147-8a64-f1775b8cdb2d', 'header_path': '/'}, hash='0f1fc6bd31c1a8dffdcdcab398ee7d0de5fc48d9e2458d9a75824458dfcf6762')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='# Education', mimetype='text/plain', start_char_idx=0, end_char_idx=11, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.1991522),\n",
       " NodeWithScore(node=TextNode(id_='fa5db702-2f47-41ed-9483-d56e55e6ce3f', embedding=None, metadata={'skills': ['Process improvement', 'Program management', 'Team player', 'Data analysis', 'Effective multi-tasker', 'Results-oriented', 'Teambuilding', 'Strategic planning', 'Extensive technology experience', 'Requirements gathering', 'Project management', 'Team leader'], 'country': ['United States'], 'domain': 'IT', 'file_path': 'sampled_data\\\\IT\\\\16899268.pdf', 'pipeline_id': '14ce0924-a864-4147-8a64-f1775b8cdb2d', 'header_path': '/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8178ca53-6244-4d3b-b828-e38acea5159d', node_type='4', metadata={'skills': ['Process improvement', 'Program management', 'Team player', 'Data analysis', 'Effective multi-tasker', 'Results-oriented', 'Teambuilding', 'Strategic planning', 'Extensive technology experience', 'Requirements gathering', 'Project management', 'Team leader'], 'country': ['United States'], 'domain': 'IT', 'file_path': 'sampled_data\\\\IT\\\\16899268.pdf', 'pipeline_id': '14ce0924-a864-4147-8a64-f1775b8cdb2d'}, hash='2a681f08b9e55605f86e72fa95440533ca48fe87b8be2d6821f7571b9932d663'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='442a057a-0568-46c9-ad37-cc71b93a8952', node_type='1', metadata={'skills': ['Process improvement', 'Program management', 'Team player', 'Data analysis', 'Effective multi-tasker', 'Results-oriented', 'Teambuilding', 'Strategic planning', 'Extensive technology experience', 'Requirements gathering', 'Project management', 'Team leader'], 'country': ['United States'], 'domain': 'IT', 'file_path': 'sampled_data\\\\IT\\\\16899268.pdf', 'pipeline_id': '14ce0924-a864-4147-8a64-f1775b8cdb2d', 'header_path': '/'}, hash='768b58c380880ec48564e5f0c4be3d84dbe5dedb06410c1c335780029e07d8a3')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='# Professional Summary\\n\\nInnovative and solution focused web development manager/analyst with extensive experience in Program and Project Management. Detail-oriented and skilled in identifying technology needs, creating a plan for solving them, and leading multiple teams to implement the solutions. Self-motivated, strong leader, and team player that works hard developing staff. Experienced in working in industry and academia.', mimetype='text/plain', start_char_idx=0, end_char_idx=428, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.18257122),\n",
       " NodeWithScore(node=TextNode(id_='167c0019-89d5-4395-8667-6fd90e7eb705', embedding=None, metadata={'skills': ['Business Process Design', 'SAP', 'Dispersed Team Management'], 'country': ['United States'], 'domain': 'IT', 'file_path': 'sampled_data\\\\IT\\\\23527321.pdf', 'pipeline_id': '14ce0924-a864-4147-8a64-f1775b8cdb2d', 'header_path': '/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='352f8bdd-56a9-4e99-b9a2-a09a56abc4a9', node_type='4', metadata={'skills': ['Business Process Design', 'SAP', 'Dispersed Team Management'], 'country': ['United States'], 'domain': 'IT', 'file_path': 'sampled_data\\\\IT\\\\23527321.pdf', 'pipeline_id': '14ce0924-a864-4147-8a64-f1775b8cdb2d'}, hash='a31bc3730bd4f913c0ab596b68aacff4caeb48c2a4be168558ab03cb79ffa8ee'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c2c8a72b-d3c7-4888-83d5-93ef726878db', node_type='1', metadata={'skills': ['Business Process Design', 'SAP', 'Dispersed Team Management'], 'country': ['United States'], 'domain': 'IT', 'file_path': 'sampled_data\\\\IT\\\\23527321.pdf', 'pipeline_id': '14ce0924-a864-4147-8a64-f1775b8cdb2d', 'header_path': '/'}, hash='3bc3c223dbc530ca8348f4841e25d6be83d67bbacf33c53855585789920d6a97')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"# Manager, Commercial and Field Force Systems\\n\\nApril 1998 to January 2002 Company Name - City, State\\n\\nLed a team of developers implementing web-based solutions for support of Schering-Plough's field forces in the United States.\", mimetype='text/plain', start_char_idx=0, end_char_idx=227, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.16079243),\n",
       " NodeWithScore(node=TextNode(id_='45f753f2-9b8f-4df5-bcce-62d4eef349e7', embedding=None, metadata={'skills': ['Process improvement', 'Program management', 'Team player', 'Data analysis', 'Effective multi-tasker', 'Results-oriented', 'Teambuilding', 'Strategic planning', 'Extensive technology experience', 'Requirements gathering', 'Project management', 'Team leader'], 'country': ['United States'], 'domain': 'IT', 'file_path': 'sampled_data\\\\IT\\\\16899268.pdf', 'pipeline_id': '14ce0924-a864-4147-8a64-f1775b8cdb2d', 'header_path': '/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8178ca53-6244-4d3b-b828-e38acea5159d', node_type='4', metadata={'skills': ['Process improvement', 'Program management', 'Team player', 'Data analysis', 'Effective multi-tasker', 'Results-oriented', 'Teambuilding', 'Strategic planning', 'Extensive technology experience', 'Requirements gathering', 'Project management', 'Team leader'], 'country': ['United States'], 'domain': 'IT', 'file_path': 'sampled_data\\\\IT\\\\16899268.pdf', 'pipeline_id': '14ce0924-a864-4147-8a64-f1775b8cdb2d'}, hash='2a681f08b9e55605f86e72fa95440533ca48fe87b8be2d6821f7571b9932d663'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='74747bf2-8f9b-415a-aff3-962f15988bf1', node_type='1', metadata={'skills': ['Process improvement', 'Program management', 'Team player', 'Data analysis', 'Effective multi-tasker', 'Results-oriented', 'Teambuilding', 'Strategic planning', 'Extensive technology experience', 'Requirements gathering', 'Project management', 'Team leader'], 'country': ['United States'], 'domain': 'IT', 'file_path': 'sampled_data\\\\IT\\\\16899268.pdf', 'pipeline_id': '14ce0924-a864-4147-8a64-f1775b8cdb2d', 'header_path': '/'}, hash='8a074ff8060694aaa39f3ac5f89b085ce0c7813fe6b057aad63a68c218457c33')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='# Web Communications Manager\\n\\n01/1999 to 01/2003\\n\\nCompany Name  City, State\\n\\nDeveloped and managed websites and web software related projects while staying within the $2 million budget of the Web Communications department. Directed various development teams of project managers and programmers focusing on internal and external users. Created project plans and worked with marketing and executive leadership to gain approval for projects. Regularly worked by phone or email to complete projects.\\n\\n- Led project teams to roll out first company wide intranet, while delivering 1 month faster than original timeline.\\n- Developed new corporate website from scratch, and then led teams to roll out new website.\\n- Developed, trained, and implemented Corporate Web Design Standards across the company.', mimetype='text/plain', start_char_idx=0, end_char_idx=795, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.08570928),\n",
       " NodeWithScore(node=TextNode(id_='74747bf2-8f9b-415a-aff3-962f15988bf1', embedding=None, metadata={'skills': ['Process improvement', 'Program management', 'Team player', 'Data analysis', 'Effective multi-tasker', 'Results-oriented', 'Teambuilding', 'Strategic planning', 'Extensive technology experience', 'Requirements gathering', 'Project management', 'Team leader'], 'country': ['United States'], 'domain': 'IT', 'file_path': 'sampled_data\\\\IT\\\\16899268.pdf', 'pipeline_id': '14ce0924-a864-4147-8a64-f1775b8cdb2d', 'header_path': '/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8178ca53-6244-4d3b-b828-e38acea5159d', node_type='4', metadata={'skills': ['Process improvement', 'Program management', 'Team player', 'Data analysis', 'Effective multi-tasker', 'Results-oriented', 'Teambuilding', 'Strategic planning', 'Extensive technology experience', 'Requirements gathering', 'Project management', 'Team leader'], 'country': ['United States'], 'domain': 'IT', 'file_path': 'sampled_data\\\\IT\\\\16899268.pdf', 'pipeline_id': '14ce0924-a864-4147-8a64-f1775b8cdb2d'}, hash='2a681f08b9e55605f86e72fa95440533ca48fe87b8be2d6821f7571b9932d663'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='60ff18d9-fb99-414a-a758-e2ac1823edc5', node_type='1', metadata={'skills': ['Process improvement', 'Program management', 'Team player', 'Data analysis', 'Effective multi-tasker', 'Results-oriented', 'Teambuilding', 'Strategic planning', 'Extensive technology experience', 'Requirements gathering', 'Project management', 'Team leader'], 'country': ['United States'], 'domain': 'IT', 'file_path': 'sampled_data\\\\IT\\\\16899268.pdf', 'pipeline_id': '14ce0924-a864-4147-8a64-f1775b8cdb2d', 'header_path': '/'}, hash='7ad06e9513e4aa82fb053edbbc2359aa36398826e4e4c45f6c9e463481d0e117')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='# Manager\\n\\n06/2003 to 06/2006\\n\\nCompany Name  City, State\\n\\n- Assisted with the running of the Des Moines Store (Largest in district).\\n- Managed 25 employees.\\n- Trained employees to complete their position duties.\\n- Handled cash daily and reconciled accounts.', mimetype='text/plain', start_char_idx=0, end_char_idx=258, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.06430005)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the retrieved candidates resumes file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "idd3UoL4MlYp",
    "outputId": "852844b2-11be-4fca-854a-5e43dd1d6d88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sampled_data\\\\IT\\\\16899268.pdf', 'sampled_data\\\\IT\\\\23527321.pdf']\n"
     ]
    }
   ],
   "source": [
    "print(get_candidates_file_paths(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfBDzbhNXV09"
   },
   "source": [
    "## Retrieve candidate based on JD (Job Description)\n",
    "\n",
    "Here we retrieve candidates based on Job Description.\n",
    "\n",
    "We parse the job description pdf and use it to retrieve the relevant candidates for the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9K3-TQjHXY3i"
   },
   "source": [
    "### Parse Job Description (JD)\n",
    "\n",
    "Here, we parse the sample job_description.pdf that we have downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jVr2ZZHFMmzq",
    "outputId": "e0958b08-faa2-4a24-e0b6-875d1d85c2d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1/1: ./job_description.pdf\n",
      "Started parsing the file under job_id fae80e8f-2484-421f-b065-de0ee1a7a8c2\n"
     ]
    }
   ],
   "source": [
    "job_description_file_path = './job_description.pdf'\n",
    "\n",
    "job_description_document = parse_files([job_description_file_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "IlKmaJY6MoYG"
   },
   "outputs": [],
   "source": [
    "job_description = \"\\n\\n\".join([doc.text for doc in job_description_document[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dogo3GkNMpkd",
    "outputId": "c8a50dc9-c824-4c61-90f5-1dfcafb5d3c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Senior Information Technology Manager\n",
      "\n",
      "# About the Role\n",
      "\n",
      "We are seeking an experienced Information Technology Manager to lead our technology initiatives and drive digital transformation across the organization. The ideal candidate will combine strong technical expertise with business acumen and leadership skills.\n",
      "\n",
      "# Key Responsibilities\n",
      "\n",
      "- Lead and manage a cross-functional IT team in developing and implementing technology solutions\n",
      "- Oversee the planning, implementation, and maintenance of enterprise IT systems and infrastructure\n",
      "- Drive strategic technology initiatives aligned with business objectives\n",
      "- Manage vendor relationships and technology partnerships\n",
      "- Ensure system security, data integrity, and business continuity\n",
      "- Develop and maintain IT policies, procedures, and best practices\n",
      "- Budget planning and resource allocation for IT projects\n",
      "- Provide technical leadership in evaluating and implementing new technologies\n",
      "- Collaborate with stakeholders to identify technology needs and solutions\n",
      "\n",
      "# Required Qualifications\n",
      "\n",
      "- Bachelor's degree in Computer Science, Information Technology, or related field\n",
      "- 8+ years of progressive IT management experience\n",
      "- Strong experience with enterprise systems and infrastructure management\n",
      "- Proven track record of successful project management and delivery\n",
      "- Experience with IT security, compliance, and risk management\n",
      "- Strong knowledge of current technology trends and solutions\n",
      "\n",
      "# Technical Skills\n",
      "\n",
      "- Enterprise Resource Planning (ERP) systems\n",
      "- Network infrastructure and security\n",
      "- Cloud computing platforms and services\n",
      "\n",
      "# IT Service Management Frameworks\n",
      "\n",
      "- Database management systems\n",
      "- System integration and architecture\n",
      "- Virtualization technologies\n",
      "- Disaster recovery and business continuity\n",
      "\n",
      "# Leadership Skills\n",
      "\n",
      "- Team management and development\n",
      "- Strategic planning and execution\n",
      "- Strong communication and presentation skills\n",
      "- Problem-solving and analytical thinking\n",
      "- Change management\n",
      "- Budget management\n",
      "- Stakeholder management\n",
      "- Cross-functional collaboration\n",
      "\n",
      "# Preferred Qualifications\n",
      "\n",
      "- Master's degree in related field\n",
      "- Professional certifications (PMP, ITIL, etc.)\n",
      "- Experience with digital transformation initiatives\n",
      "- Knowledge of agile methodologies\n",
      "- Experience in similar industry\n",
      "\n",
      "# What We Offer\n",
      "\n",
      "- Competitive salary and benefits package\n",
      "- Professional development opportunities\n",
      "- Collaborative work environment\n",
      "- Opportunity to drive technological innovation\n",
      "- Work-life balance\n",
      "- Career advancement potential\n",
      "\n",
      "The ideal candidate will be a results-driven leader who can balance technical expertise with business strategy, while effectively managing teams and stakeholders across the organization.\n"
     ]
    }
   ],
   "source": [
    "print(job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaQq0v4xXdHg"
   },
   "source": [
    "### Retrieve candidates\n",
    "\n",
    "Here we retrieve candidates based on the job description text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eR1ZHslDMtZJ",
    "outputId": "f9f1aafe-247e-4b35-a346-a6992e28b2d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Inferred filters: {\"filters\":[{\"key\":\"domain\",\"value\":\"IT\",\"operator\":\"==\"},{\"key\":\"country\",\"value\":[\"United States\"],\"operator\":\"in\"},{\"key\":\"skills\",\"value\":[\"Enterprise Resource Planning (ERP) systems\",\"Network infrastructure and security\",\"Cloud computing platforms and services\",\"Database management systems\",\"System integration and architecture\",\"Virtualization technologies\",\"Disaster recovery and business continuity\",\"Team management and development\",\"Strategic planning and execution\",\"Strong communication and presentation skills\",\"Problem-solving and analytical thinking\",\"Change management\",\"Budget management\",\"Stakeholder management\",\"Cross-functional collaboration\"],\"operator\":\"in\"}],\"condition\":\"or\"}\n"
     ]
    }
   ],
   "source": [
    "candidates_based_on_jd = await candidates_retriever_from_jd(job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_hdQbXCMvSw"
   },
   "outputs": [],
   "source": [
    "candidates_file_paths = get_candidates_file_paths(candidates_based_on_jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r5DFzje1P1-C",
    "outputId": "8df11b4b-0cc7-46be-fdb0-a21020c6b898"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sampled_data/IT/18159866.pdf', 'sampled_data/FINANCE/25101183.pdf', 'sampled_data/IT/27536013.pdf']\n"
     ]
    }
   ],
   "source": [
    "print(candidates_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBJaAwAlXiKL"
   },
   "source": [
    "## Analyse candidate resume based on retrieval\n",
    "\n",
    "Once we have the relevant candidate resumes, we need to analyze why, how, and which candidates are suitable for the job description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7YmWH3uXm7p"
   },
   "source": [
    "### Parse the candidate resumes\n",
    "\n",
    "Here, we parse the candidate resumes retrieved based on the job description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3LxRhGjQP4ml",
    "outputId": "f200ba2f-5a34-4f9b-9d9d-d661d0cb5fea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1/3: sampled_data/IT/18159866.pdf\n",
      "Started parsing the file under job_id 50c1cf22-b692-4521-b40c-62f6a31e1215\n",
      "Processing file 2/3: sampled_data/FINANCE/25101183.pdf\n",
      "Started parsing the file under job_id 85992196-4c31-474c-8423-3ead6fe5835f\n",
      "Processing file 3/3: sampled_data/IT/27536013.pdf\n",
      "Started parsing the file under job_id e4263df3-4611-453c-834d-3c0eecf522be\n"
     ]
    }
   ],
   "source": [
    "candidates_resumes = parse_files(candidates_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YAcJ2b0P9g1"
   },
   "outputs": [],
   "source": [
    "candidates_resumes_text = \"\\n\\n\".join([doc.text for docs in candidates_resumes for doc in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BX27yyvXpeP"
   },
   "source": [
    "### Analyses\n",
    "\n",
    "Let's analyze the candidate resumes against the job description by processing them through the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "of6qb4S_Qb4J"
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"Based on the following job description, please share the analysis of why specific candidates are suitable for the job.\n",
    "\n",
    "        Job Description:\n",
    "        {job_description}\n",
    "\n",
    "        Candidates:\n",
    "        {candidates_resumes_text}\n",
    "        \"\"\"\n",
    "\n",
    "analyses = llm.complete(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyGtOSpSQx_x",
    "outputId": "d73e4559-2db4-404e-958c-978064977bdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the job description for the Senior Information Technology Manager position and the profiles of the candidates provided, here is an analysis of why specific candidates may be suitable for the job:\n",
      "\n",
      "### Candidate 1: Senior Vice President of Global Information Technology\n",
      "\n",
      "**Strengths:**\n",
      "1. **Extensive Experience:** With over 20 years in IT management, including a current role as Senior Vice President, this candidate has significant experience leading large teams and managing complex IT environments, which aligns well with the requirement for 8+ years of progressive IT management experience.\n",
      "   \n",
      "2. **Project Management Expertise:** The candidate has a proven track record of managing cross-functional teams on large implementations and development projects, which is crucial for overseeing the planning, implementation, and maintenance of enterprise IT systems.\n",
      "\n",
      "3. **Strategic Planning and Execution:** Their experience in strategic planning and change implementation demonstrates the ability to drive strategic technology initiatives aligned with business objectives.\n",
      "\n",
      "4. **Risk Management and Security:** The candidate has conducted periodic risk assessments and developed disaster recovery plans, which is essential for ensuring system security, data integrity, and business continuity.\n",
      "\n",
      "5. **Cost Savings and Efficiency:** The candidate has a history of delivering projects on time and within budget, realizing significant improvements in processing efficiency, which is a key responsibility in the job description.\n",
      "\n",
      "**Suitability:** This candidate is highly suitable due to their extensive leadership experience, strategic planning capabilities, and proven success in managing large-scale IT projects and teams.\n",
      "\n",
      "---\n",
      "\n",
      "### Candidate 2: Program Manager / PMO Director\n",
      "\n",
      "**Strengths:**\n",
      "1. **Program Management Experience:** This candidate has a strong background in program management and has led multi-functional technology teams, which is essential for managing a cross-functional IT team.\n",
      "\n",
      "2. **Financial Acumen:** Their experience in managing budgets, financial analysis, and cost savings initiatives aligns well with the job's requirement for budget planning and resource allocation for IT projects.\n",
      "\n",
      "3. **Change Management:** The candidate has a proven ability to turn around underperforming programs and lead successful change initiatives, which is important for driving digital transformation across the organization.\n",
      "\n",
      "4. **Stakeholder Management:** They have demonstrated skills in stakeholder management and collaboration, which are critical for identifying technology needs and solutions in collaboration with various stakeholders.\n",
      "\n",
      "5. **Certifications:** The candidate holds relevant certifications such as PMP and ITIL, which are preferred qualifications for the role.\n",
      "\n",
      "**Suitability:** This candidate is suitable due to their strong program management skills, financial expertise, and experience in leading change initiatives, making them a good fit for the strategic and operational aspects of the role.\n",
      "\n",
      "---\n",
      "\n",
      "### Candidate 3: Experienced Information Technology Manager\n",
      "\n",
      "**Strengths:**\n",
      "1. **Diverse IT Management Experience:** With over 10 years of experience in various management areas, this candidate has a solid foundation in IT management, which meets the job's experience requirement.\n",
      "\n",
      "2. **Project Management Skills:** The candidate has demonstrated proficiency in project management, managing resources, and personnel, which is essential for leading and managing an IT team.\n",
      "\n",
      "3. **Business Intelligence and Reporting:** Their experience in redesigning BI programs and implementing effective systems indicates a strong understanding of technology solutions that can drive business objectives.\n",
      "\n",
      "4. **User Relations and Training:** The candidate has experience in user relations and training, which is important for collaborating with stakeholders to identify technology needs and solutions.\n",
      "\n",
      "5. **Cost Savings Initiatives:** They have successfully implemented projects that resulted in significant cost savings, aligning with the job's focus on budget management and resource allocation.\n",
      "\n",
      "**Suitability:** This candidate is suitable due to their solid IT management experience, project management skills, and ability to implement effective technology solutions that align with business needs.\n",
      "\n",
      "---\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Each candidate brings a unique set of skills and experiences that align with the requirements of the Senior Information Technology Manager position. Candidate 1 stands out for their extensive leadership experience and strategic planning capabilities. Candidate 2 offers strong program management and financial acumen, while Candidate 3 provides a solid foundation in IT management and project execution. Depending on the specific needs and culture of the organization, any of these candidates could be a strong fit for the role.\n"
     ]
    }
   ],
   "source": [
    "print(analyses)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
