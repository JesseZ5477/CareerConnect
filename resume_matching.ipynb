{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86XWvjxeZV8I"
   },
   "source": [
    "# LLM-Native Resume Matching Solution\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/run-llama/llamacloud-demo/blob/main/examples/resume_matching/resume_matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "This notebook demonstrates the implementation of an LLM-native resume matching solution that transforms traditional resume screening into an AI-powered, conversational experience. This aims to streamline the recruitment process by automating candidate matching and providing natural language interaction for recruiters.\n",
    "\n",
    "## Use Case Overview\n",
    "- **Problem**: Traditional resume screening relies heavily on manual filter selection and explicit matching criteria, making it inefficient and time-consuming for recruiters.\n",
    "- **Solution**: An LLM-native approach that uses generative AI to:\n",
    "  - Extract structured information from resumes automatically\n",
    "  - Enable natural language queries for candidate search\n",
    "  - Provide matching between job descriptions and candidates\n",
    "  - Offer detailed analysis of why candidates match specific roles\n",
    "\n",
    "## Implementation Steps\n",
    "1. **Data Processing**\n",
    "   - Parse PDF resumes using LlamaParse\n",
    "   - Extract structured metadata (skills, education, domain) using LLMs\n",
    "   - Store processed documents in LlamaCloud for efficient retrieval\n",
    "\n",
    "2. **Index Creation**\n",
    "   - Create a Pipeline/ Index using LlamaCloud\n",
    "   - Configure embedding and transformation settings\n",
    "   - Upload processed documents with metadata\n",
    "\n",
    "3. **Query Processing**\n",
    "   - Support two types of queries:\n",
    "     - Natural language queries from recruiters (e.g., \"Find Java developers from US universities\")\n",
    "     - Job description-based matching\n",
    "   - Extract relevant metadata filters from queries using LLMs\n",
    "   - Retrieve matching candidates based on metadata and semantic search\n",
    "\n",
    "4. **Candidate Analysis**\n",
    "   - Generate detailed analysis of why candidates match job requirements\n",
    "   - Compare candidate qualifications against job criteria\n",
    "   - Provide insights into strengths and potential gaps.\n",
    "\n",
    "\n",
    "**NOTE**: For this demonstration, I have used a sample dataset consisting of 30 resumes (10 each from Information Technology, Sales, and Finance domains) sourced from the [Kaggle Resume Dataset](https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset). This smaller dataset allows for easier experimentation and clearer demonstration of the concept.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqSD3115Uoep"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Here we install `llama-index`, `llama-index-indices-managed-llama-cloud`, `llama-parse` and `llama-cloud`. \n",
    "\n",
    "These packages are tools for building, parsing, and managing LLM applications on LlamaIndex's cloud platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tbgPfxqILy5e",
    "outputId": "a28f1b7e-1ce5-45b5-c75e-080d15b70238"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Downloading llama_index-0.12.10-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-indices-managed-llama-cloud\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-parse\n",
      "  Downloading llama_parse-0.5.19-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting llama-cloud\n",
      "  Downloading llama_cloud-0.1.8-py3-none-any.whl.metadata (860 bytes)\n",
      "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_agent_openai-0.4.1-py3-none-any.whl.metadata (726 bytes)\n",
      "Collecting llama-index-cli<0.5.0,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_cli-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.13.0,>=0.12.10 (from llama-index)\n",
      "  Downloading llama_index_core-0.12.10.post1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
      "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_llms_openai-0.3.13-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.4.2-py3-none-any.whl.metadata (726 bytes)\n",
      "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
      "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_readers_file-0.4.3-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting nltk>3.8.1 (from llama-index)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click<9.0.0,>=8.1.7 (from llama-parse)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pydantic!=2.10 (from llama-parse)\n",
      "  Downloading pydantic-2.10.5-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting certifi<2025.0.0,>=2024.7.4 (from llama-cloud)\n",
      "  Downloading certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting httpx>=0.20.0 (from llama-cloud)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\jz133\\.conda\\envs\\careerconnect\\lib\\site-packages (from click<9.0.0,>=8.1.7->llama-parse) (0.4.6)\n",
      "Collecting anyio (from httpx>=0.20.0->llama-cloud)\n",
      "  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.20.0->llama-cloud)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx>=0.20.0->llama-cloud)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.20.0->llama-cloud)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading openai-1.59.6-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting PyYAML>=6.0.1 (from llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading SQLAlchemy-2.0.37-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.6 (from llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading aiohttp-3.11.11-cp312-cp312-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading Deprecated-1.2.15-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fsspec>=2023.5.0 (from llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\jz133\\.conda\\envs\\careerconnect\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (1.6.0)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting numpy (from llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading numpy-2.2.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting pillow>=9.0.0 (from llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading pillow-11.1.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting requests>=2.31.0 (from llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.2.0 (from llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading tiktoken-0.8.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting tqdm<5.0.0,>=4.66.1 (from llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\jz133\\.conda\\envs\\careerconnect\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (4.12.2)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading wrapt-1.17.1-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pandas (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting joblib (from nltk>3.8.1->llama-index)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk>3.8.1->llama-index)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=2.10->llama-parse)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic!=2.10->llama-parse)\n",
      "  Downloading pydantic_core-2.27.2-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading attrs-24.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading frozenlist-1.5.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading multidict-6.1.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading propcache-0.2.1-cp312-cp312-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading yarl-1.18.3-cp312-cp312-win_amd64.whl.metadata (71 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading jiter-0.8.2-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting sniffio (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading greenlet-3.1.1-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
      "  Downloading marshmallow-3.25.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jz133\\.conda\\envs\\careerconnect\\lib\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\jz133\\.conda\\envs\\careerconnect\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.10->llama-index) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jz133\\.conda\\envs\\careerconnect\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n",
      "Downloading llama_index-0.12.10-py3-none-any.whl (6.8 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl (11 kB)\n",
      "Downloading llama_parse-0.5.19-py3-none-any.whl (15 kB)\n",
      "Downloading llama_cloud-0.1.8-py3-none-any.whl (247 kB)\n",
      "Downloading certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading llama_index_agent_openai-0.4.1-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_cli-0.4.0-py3-none-any.whl (27 kB)\n",
      "Downloading llama_index_core-0.12.10.post1-py3-none-any.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   --------------------------------- ------ 1.3/1.6 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 5.6 MB/s eta 0:00:00\n",
      "Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading llama_index_llms_openai-0.3.13-py3-none-any.whl (14 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.4.2-py3-none-any.whl (5.9 kB)\n",
      "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
      "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_file-0.4.3-py3-none-any.whl (38 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 5.7 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.10.5-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.0/2.0 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/2.0 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 2.8 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.11.11-cp312-cp312-win_amd64.whl (437 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Downloading Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 0.8/1.7 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 4.5 MB/s eta 0:00:00\n",
      "Downloading openai-1.59.6-py3-none-any.whl (454 kB)\n",
      "Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading pillow-11.1.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.8/2.6 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.1/2.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 5.1 MB/s eta 0:00:00\n",
      "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "Downloading PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading SQLAlchemy-2.0.37-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.8/2.1 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 6.2 MB/s eta 0:00:00\n",
      "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading tiktoken-0.8.0-cp312-cp312-win_amd64.whl (883 kB)\n",
      "   ---------------------------------------- 0.0/883.8 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 262.1/883.8 kB ? eta -:--:--\n",
      "   ---------------------- --------------- 524.3/883.8 kB 837.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 883.8/883.8 kB 1.5 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading wrapt-1.17.1-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading numpy-2.2.1-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/12.6 MB 7.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.6 MB 5.8 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.1/12.6 MB 5.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.9/12.6 MB 4.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.5/12.6 MB 4.5 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 5.0/12.6 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.8/12.6 MB 4.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.8/12.6 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.9/12.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.9/12.6 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.7/12.6 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.2/12.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.3/12.6 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.1/12.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 4.1 MB/s eta 0:00:00\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/11.5 MB 4.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.6/11.5 MB 4.4 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.6/11.5 MB 4.4 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.1/11.5 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.9/11.5 MB 3.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.5/11.5 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.0/11.5 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.3/11.5 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.3/11.5 MB 3.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.1/11.5 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.2/11.5 MB 4.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.2/11.5 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.5 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 4.1 MB/s eta 0:00:00\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-24.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading frozenlist-1.5.0-cp312-cp312-win_amd64.whl (51 kB)\n",
      "Downloading greenlet-3.1.1-cp312-cp312-win_amd64.whl (299 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading jiter-0.8.2-cp312-cp312-win_amd64.whl (204 kB)\n",
      "Downloading marshmallow-3.25.1-py3-none-any.whl (49 kB)\n",
      "Downloading multidict-6.1.0-cp312-cp312-win_amd64.whl (28 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading propcache-0.2.1-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Downloading yarl-1.18.3-cp312-cp312-win_amd64.whl (90 kB)\n",
      "Installing collected packages: striprtf, pytz, filetype, dirtyjson, wrapt, urllib3, tzdata, tqdm, tenacity, soupsieve, sniffio, regex, PyYAML, pypdf, pydantic-core, propcache, pillow, numpy, networkx, mypy-extensions, multidict, marshmallow, joblib, jiter, idna, h11, greenlet, fsspec, frozenlist, distro, click, charset-normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests, pydantic, pandas, nltk, httpcore, deprecated, beautifulsoup4, anyio, aiosignal, tiktoken, httpx, dataclasses-json, aiohttp, openai, llama-index-core, llama-cloud, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.37 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 annotated-types-0.7.0 anyio-4.8.0 attrs-24.3.0 beautifulsoup4-4.12.3 certifi-2024.12.14 charset-normalizer-3.4.1 click-8.1.8 dataclasses-json-0.6.7 deprecated-1.2.15 dirtyjson-1.0.8 distro-1.9.0 filetype-1.2.0 frozenlist-1.5.0 fsspec-2024.12.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 idna-3.10 jiter-0.8.2 joblib-1.4.2 llama-cloud-0.1.8 llama-index-0.12.10 llama-index-agent-openai-0.4.1 llama-index-cli-0.4.0 llama-index-core-0.12.10.post1 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.3 llama-index-llms-openai-0.3.13 llama-index-multi-modal-llms-openai-0.4.2 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.3 llama-index-readers-llama-parse-0.4.0 llama-parse-0.5.19 marshmallow-3.25.1 multidict-6.1.0 mypy-extensions-1.0.0 networkx-3.4.2 nltk-3.9.1 numpy-2.2.1 openai-1.59.6 pandas-2.2.3 pillow-11.1.0 propcache-0.2.1 pydantic-2.10.5 pydantic-core-2.27.2 pypdf-5.1.0 pytz-2024.2 regex-2024.11.6 requests-2.32.3 sniffio-1.3.1 soupsieve-2.6 striprtf-0.0.26 tenacity-9.0.0 tiktoken-0.8.0 tqdm-4.67.1 typing-inspect-0.9.0 tzdata-2024.2 urllib3-2.3.0 wrapt-1.17.1 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -U llama-index llama-index-indices-managed-llama-cloud llama-parse llama-cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yr0EAvWqMCx5"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abREdlc9Us-5"
   },
   "source": [
    "## Setup API Keys\n",
    "\n",
    "We will utilize `gpt-4o-mini` from OpenAI's LLM and our LlamaCloud, an enterprise platform designed for building LLM applications.\n",
    "\n",
    "Here, we will set up the `OPENAI_API_KEY` and `LLAMA_CLOUD_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tG3WZeDYMF8h"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\" # Get your API key from https://platform.openai.com/account/api-keys\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-ulskyur4sbyH90bJrcJSAzP4BdY6WqtxljOL4zPiXR2WvuVi\" # Get your API key from https://cloud.llamaindex.ai/api-key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeVZhD86UvXc"
   },
   "source": [
    "## Setup LLM\n",
    "\n",
    "We will initialize `gpt-4o-mini` OpenAI LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oI-rcTFWMHTk"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Files\n",
    "\n",
    "We will download sampled data from [Kaggle Resume Dataset](https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset) and `job_description.pdf`.\n",
    "\n",
    "`sampled_dataset` - contains 10 each from Information Technology, Sales, and Finance domains.\n",
    "\n",
    "`job_description.pdf` - This is the job description file we will use to retrieve candidate profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Download the sampled data\n",
    "!wget --content-disposition \"https://www.dropbox.com/scl/fo/v1mn1rxqz2ifqtx009owh/APHC7xPTQ7BiRZv0BKZ7cag?rlkey=rh09o73172vzifjqlsmw4fhmo&st=v220giff&dl=1\"\n",
    "\n",
    "# make a directory to store the data\n",
    "!mkdir -p \"./sampled_data\"\n",
    "\n",
    "# unzip the data\n",
    "!unzip sampled_data.zip -d \"./sampled_data\"\n",
    "\n",
    "# Download the job description file\n",
    "!wget -O job_description.pdf \"https://www.dropbox.com/scl/fi/b1djiczj6vy8s6h4isvmr/job_description.pdf?rlkey=drpkd2exj8edkuw1f0evhvqfx&st=2i2wb801&dl=1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHwuhLUBUxkj"
   },
   "source": [
    "## Utils\n",
    "\n",
    "Here we define some functions for further processing.\n",
    "\n",
    "1. `parse_files`: Processes PDF files using LlamaParse and converts them to markdown format with updated metadata\n",
    "\n",
    "2. `list_pdf_files`: Recursively finds all PDF files in a directory and its subdirectories\n",
    "\n",
    "3. `Metadata`: Pydantic model to structure resume metadata including domain, skills, and educational country information.\n",
    "\n",
    "4. `create_llamacloud_pipeline`: Creates or updates a LlamaCloud pipeline with specified configurations.\n",
    "\n",
    "5. `get_metadata`: Extracts structured metadata from resume text using an LLM.\n",
    "\n",
    "6. `get_document_upload`: Prepares a document for cloud upload by combining text and extracted metadata.\n",
    "\n",
    "7. `upload_documents`: Batch uploads documents to LlamaCloud pipeline with parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6-pM24SBMNse"
   },
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "from pathlib import Path\n",
    "from llama_index.core import Document\n",
    "from llama_cloud.types import CloudDocumentCreate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from llama_cloud.client import LlamaCloud\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.async_utils import run_jobs\n",
    "\n",
    "def parse_files(pdf_files):\n",
    "    \"\"\"Function to parse the pdf files using LlamaParse in markdown format\"\"\"\n",
    "\n",
    "    parser = LlamaParse(\n",
    "        result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n",
    "        num_workers=4,  # if multiple files passed, split in `num_workers` API calls\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for index, pdf_file in enumerate(pdf_files):\n",
    "        print(f\"Processing file {index + 1}/{len(pdf_files)}: {pdf_file}\")\n",
    "        # Parsing and chunking pdf\n",
    "        docs = parser.load_data(pdf_file)\n",
    "        # Updating metadata with filepath\n",
    "        for doc in docs:\n",
    "          doc.metadata.update({'filepath': pdf_file})\n",
    "        documents.append(docs)\n",
    "\n",
    "    return documents\n",
    "\n",
    "def list_pdf_files(directory):\n",
    "    # List all .pdf files recursively using pathlib\n",
    "    # rglob ('recursive glob') searches through all subdirectories\n",
    "    pdf_files = [str(file) for file in Path(directory).rglob('*.pdf')]\n",
    "    return pdf_files\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    \"\"\"\n",
    "    A data model representing key professional and educational metadata extracted from a resume.\n",
    "    This class captures essential candidate information including technical/professional skills\n",
    "    and the geographical distribution of their educational background.\n",
    "\n",
    "    Attributes:\n",
    "        skills (List[str]): Technical and professional competencies of the candidate\n",
    "        country (List[str]): Countries where the candidate pursued formal education\n",
    "\n",
    "    Example:\n",
    "        {\n",
    "            \"skills\": [\"Python\", \"Machine Learning\", \"SQL\", \"Project Management\"],\n",
    "            \"country\": [\"United States\", \"India\"],\n",
    "            \"domain\": \"Information Technology\"\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    domain: str = Field(...,\n",
    "                        description=\"The domain of the candidate can be one of SALES/ IT/ FINANCE\"\n",
    "                                    \"Returns an empty string if no domain is identified.\")\n",
    "\n",
    "    skills: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"List of technical, professional, and soft skills extracted from the resume. \"\n",
    "                   \"and domain expertise. Returns an empty list if no skills are identified.\"\n",
    "    )\n",
    "\n",
    "    country: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"List of countries where the candidate completed their formal education, Only extract the country.\"\n",
    "                   \"Returns an empty list if countries are not specified.\"\n",
    "    )\n",
    "\n",
    "def create_llamacloud_pipeline(pipeline_name, embedding_config, transform_config, data_sink_id=None):\n",
    "    \"\"\"Function to create a pipeline in llamacloud\"\"\"\n",
    "\n",
    "    client = LlamaCloud(token=os.environ[\"LLAMA_CLOUD_API_KEY\"])\n",
    "\n",
    "    pipeline = {\n",
    "        'name': pipeline_name,\n",
    "        'transform_config': transform_config,\n",
    "        'embedding_config': embedding_config,\n",
    "        'data_sink_id': data_sink_id\n",
    "    }\n",
    "\n",
    "    pipeline = client.pipelines.upsert_pipeline(request=pipeline)\n",
    "\n",
    "    return client, pipeline\n",
    "\n",
    "async def get_metadata(text):\n",
    "    \"\"\"Function to get the metadata from the given resume of the candidate\"\"\"\n",
    "    prompt_template = PromptTemplate(\"\"\"Generate skills, and country of the education for the given candidate resume.\n",
    "\n",
    "    Resume of the candidate:\n",
    "\n",
    "    {text}\"\"\")\n",
    "\n",
    "    metadata = await llm.astructured_predict(\n",
    "        Metadata,\n",
    "        prompt_template,\n",
    "        text=text,\n",
    "    )\n",
    "\n",
    "    return metadata\n",
    "\n",
    "async def get_document_upload(documents, llm):\n",
    "    full_text = \"\\n\\n\".join([doc.text for doc in documents])\n",
    "\n",
    "    # Get the file path of the resume\n",
    "    file_path = documents[0].metadata['filepath']\n",
    "\n",
    "    # Extract metadata from the resume\n",
    "    extracted_metadata = await get_metadata(full_text)\n",
    "\n",
    "    skills = list(set(getattr(extracted_metadata, 'skills', [])))\n",
    "    country = list(set(getattr(extracted_metadata, 'country', [])))\n",
    "    domain = getattr(extracted_metadata, 'domain', '')\n",
    "\n",
    "    global_skills.extend(skills)\n",
    "    global_countries.extend(country)\n",
    "    global_domains.append(domain)\n",
    "\n",
    "    return CloudDocumentCreate(\n",
    "                text=full_text,\n",
    "                metadata={\n",
    "                    'skills': skills,\n",
    "                    'country': country,\n",
    "                    'domain': domain,\n",
    "                    'file_path': file_path\n",
    "                }\n",
    "            )\n",
    "\n",
    "async def upload_documents(client, pipeline, documents):\n",
    "    \"\"\"Function to upload the documents to the cloud\"\"\"\n",
    "\n",
    "    # Upload the documents to the cloud\n",
    "    extract_jobs = []\n",
    "    for doc in documents:\n",
    "        extract_jobs.append(get_document_upload(doc, llm))\n",
    "\n",
    "    documents_upload_objs = await run_jobs(extract_jobs, workers=4)\n",
    "\n",
    "    _ = client.pipelines.create_batch_pipeline_documents(pipeline.id, request=documents_upload_objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSaUjbxuVrbI"
   },
   "source": [
    "## Parse the files\n",
    "\n",
    "Here, we get a list of files from the `sampled_data` directory and parse them using `LlamaParse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zQsRIXOkMZKf",
    "outputId": "a8419c5e-25b4-45fc-ecd5-19d7e73b06a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1/30: sampled_data\\FINANCE\\12071138.pdf\n",
      "Error while parsing the file 'sampled_data\\FINANCE\\12071138.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 2/30: sampled_data\\FINANCE\\14408510.pdf\n",
      "Error while parsing the file 'sampled_data\\FINANCE\\14408510.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 3/30: sampled_data\\FINANCE\\15891494.pdf\n",
      "Error while parsing the file 'sampled_data\\FINANCE\\15891494.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 4/30: sampled_data\\FINANCE\\19540089.pdf\n",
      "Error while parsing the file 'sampled_data\\FINANCE\\19540089.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 5/30: sampled_data\\FINANCE\\24967652.pdf\n",
      "Error while parsing the file 'sampled_data\\FINANCE\\24967652.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 6/30: sampled_data\\FINANCE\\25101183.pdf\n",
      "Error while parsing the file 'sampled_data\\FINANCE\\25101183.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 7/30: sampled_data\\FINANCE\\26961846.pdf\n",
      "Error while parsing the file 'sampled_data\\FINANCE\\26961846.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 8/30: sampled_data\\FINANCE\\28398216.pdf\n",
      "Error while parsing the file 'sampled_data\\FINANCE\\28398216.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 9/30: sampled_data\\FINANCE\\38907798.pdf\n",
      "Error while parsing the file 'sampled_data\\FINANCE\\38907798.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 10/30: sampled_data\\FINANCE\\84373843.pdf\n",
      "Error while parsing the file 'sampled_data\\FINANCE\\84373843.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 11/30: sampled_data\\IT\\16899268.pdf\n",
      "Error while parsing the file 'sampled_data\\IT\\16899268.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 12/30: sampled_data\\IT\\17641670.pdf\n",
      "Error while parsing the file 'sampled_data\\IT\\17641670.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 13/30: sampled_data\\IT\\18159866.pdf\n",
      "Error while parsing the file 'sampled_data\\IT\\18159866.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 14/30: sampled_data\\IT\\23527321.pdf\n",
      "Error while parsing the file 'sampled_data\\IT\\23527321.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 15/30: sampled_data\\IT\\25959103.pdf\n",
      "Error while parsing the file 'sampled_data\\IT\\25959103.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 16/30: sampled_data\\IT\\27536013.pdf\n",
      "Error while parsing the file 'sampled_data\\IT\\27536013.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 17/30: sampled_data\\IT\\38753827.pdf\n",
      "Error while parsing the file 'sampled_data\\IT\\38753827.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 18/30: sampled_data\\IT\\52246737.pdf\n",
      "Error while parsing the file 'sampled_data\\IT\\52246737.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 19/30: sampled_data\\IT\\52618188.pdf\n",
      "Error while parsing the file 'sampled_data\\IT\\52618188.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 20/30: sampled_data\\IT\\57002858.pdf\n",
      "Error while parsing the file 'sampled_data\\IT\\57002858.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 21/30: sampled_data\\SALES\\12696104.pdf\n",
      "Error while parsing the file 'sampled_data\\SALES\\12696104.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 22/30: sampled_data\\SALES\\17509935.pdf\n",
      "Error while parsing the file 'sampled_data\\SALES\\17509935.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 23/30: sampled_data\\SALES\\17704246.pdf\n",
      "Error while parsing the file 'sampled_data\\SALES\\17704246.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 24/30: sampled_data\\SALES\\19473948.pdf\n",
      "Error while parsing the file 'sampled_data\\SALES\\19473948.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 25/30: sampled_data\\SALES\\25315791.pdf\n",
      "Error while parsing the file 'sampled_data\\SALES\\25315791.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 26/30: sampled_data\\SALES\\28198029.pdf\n",
      "Error while parsing the file 'sampled_data\\SALES\\28198029.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 27/30: sampled_data\\SALES\\30608780.pdf\n",
      "Error while parsing the file 'sampled_data\\SALES\\30608780.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 28/30: sampled_data\\SALES\\31199035.pdf\n",
      "Error while parsing the file 'sampled_data\\SALES\\31199035.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 29/30: sampled_data\\SALES\\33236701.pdf\n",
      "Error while parsing the file 'sampled_data\\SALES\\33236701.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "Processing file 30/30: sampled_data\\SALES\\55097118.pdf\n",
      "Error while parsing the file 'sampled_data\\SALES\\55097118.pdf': [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n"
     ]
    }
   ],
   "source": [
    "directory = './sampled_data/'\n",
    "pdf_files = list_pdf_files(directory)\n",
    "\n",
    "documents = parse_files(pdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_gtZk0ZVtrS"
   },
   "source": [
    "## Let's keep a track of skills, countries and domains.\n",
    "\n",
    "We will track `skills`, `countries`, and `domains` in each parsed resume.\n",
    "\n",
    "Here, we will initialize lists for `global_skills`, `global_countries`, and `global_domains` to monitor these attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvdY7FLeMbL9"
   },
   "outputs": [],
   "source": [
    "global_skills = []\n",
    "global_countries = []\n",
    "global_domains = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYzBr6tsWVoI"
   },
   "source": [
    "## Create LlamaCloud Pipeline/ Index\n",
    "\n",
    "Here, we define `embedding_config` and `transform_config` to set the `OPENAI_EMBEDDING`, `chunk_size`, and `chunk_overlap` parameters needed for creating an index on `LlamaCloud`.\n",
    "\n",
    "We will then create a pipeline/index on `LlamaCloud` under the name `resume_matching`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLh_E7AyMdsq"
   },
   "outputs": [],
   "source": [
    "# Embedding config\n",
    "embedding_config = {\n",
    "    'type': 'OPENAI_EMBEDDING',\n",
    "    'component': {\n",
    "        'api_key': os.environ[\"OPENAI_API_KEY\"], # editable\n",
    "        'model_name': 'text-embedding-ada-002' # editable\n",
    "    }\n",
    "}\n",
    "\n",
    "# Transformation auto config\n",
    "transform_config = {\n",
    "    'mode': 'auto',\n",
    "    'config': {\n",
    "        'chunk_size': 1024,\n",
    "        'chunk_overlap': 20\n",
    "    }\n",
    "}\n",
    "\n",
    "client, pipeline = create_llamacloud_pipeline('resume_matching', embedding_config, transform_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTNqPyNZWYuC"
   },
   "source": [
    "## Upload Documents\n",
    "\n",
    "Once the index/pipeline is created, we will upload all the parsed resumes (documents) using the `upload_documents` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpHcng41MfS4"
   },
   "outputs": [],
   "source": [
    "await upload_documents(client, pipeline, documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efO7c5IHWa4S"
   },
   "source": [
    "## Connect to LlamaCloud Index\n",
    "\n",
    "Here, we connect to the `resume_matching` index that was created on `LlamaCloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2AVm31UbMgnZ"
   },
   "outputs": [],
   "source": [
    "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
    "\n",
    "index = LlamaCloudIndex(\n",
    "  name=\"resume_matching\",\n",
    "  project_name=\"Default\",\n",
    "  organization_id=\"YOUR ORGANIZATION ID\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "An5NXm0mXBql"
   },
   "source": [
    "## Utils for Candidate retrieval.\n",
    "\n",
    "Once the index is created, we need to retrieve candidate profiles based on HR queries. Here, we will define some functions for this purpose.\n",
    "\n",
    "1. `get_query_metadata`: Extracts structured metadata from user queries by matching against existing global metadata\n",
    "\n",
    "2. `candidates_retriever_from_query`: Retrieves relevant candidate profiles based on user query using metadata filters\n",
    "\n",
    "3. `get_candidates_file_paths`: Extracts unique file paths from retrieved candidate metadata\n",
    "\n",
    "4. `candidates_retriever_from_jd`: Retrieves matching candidate profiles based on job description using metadata filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idSIbIS_MiDC"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import (\n",
    "    MetadataFilter,\n",
    "    MetadataFilters,\n",
    "    FilterOperator,\n",
    "    FilterCondition\n",
    ")\n",
    "async def get_query_metadata(text):\n",
    "    \"\"\"Function to get the metadata from the given user query\"\"\"\n",
    "    prompt_template = PromptTemplate(\"\"\"Generate skills, and country of the education for the given user query.\n",
    "\n",
    "    Extracted metadata should be from the following items:\n",
    "\n",
    "    skills: {global_skills}\n",
    "    countries: {global_countries}\n",
    "    domains: {global_domains}\n",
    "    user query:\n",
    "\n",
    "    {text}\"\"\")\n",
    "\n",
    "    extracted_metadata = await llm.astructured_predict(\n",
    "        Metadata,\n",
    "        prompt_template,\n",
    "        text=text,\n",
    "        global_skills=global_skills,\n",
    "        global_countries=global_countries,\n",
    "        global_domains=global_domains\n",
    "    )\n",
    "\n",
    "    return extracted_metadata\n",
    "\n",
    "async def candidates_retriever_from_query(query: str):\n",
    "    \"\"\"Synthesizes an answer to your question by feeding in an entire relevant document as context.\"\"\"\n",
    "    print(f\"> User query string: {query}\")\n",
    "    # Use structured predict to infer the metadata filters and query string.\n",
    "    metadata_info = await get_query_metadata(query)\n",
    "    filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(key=\"domain\", operator=FilterOperator.EQ, value=metadata_info.domain),\n",
    "        MetadataFilter(key=\"country\", operator=FilterOperator.IN, value=metadata_info.country),\n",
    "        MetadataFilter(key=\"skills\", operator=FilterOperator.IN, value=metadata_info.skills)\n",
    "    ],\n",
    "    condition=FilterCondition.OR\n",
    ")\n",
    "    print(f\"> Inferred filters: {filters.json()}\")\n",
    "    retriever = index.as_retriever(\n",
    "    retrieval_mode=\"chunks\",\n",
    "    metadata_filters=filters,\n",
    "    )\n",
    "    # run query\n",
    "    return retriever.retrieve(query)\n",
    "\n",
    "def get_candidates_file_paths(candidates):\n",
    "\n",
    "  file_paths = []\n",
    "  for candidate in candidates:\n",
    "    file_paths.append(candidate.metadata['file_path'])\n",
    "\n",
    "  return list(set(file_paths))\n",
    "\n",
    "async def candidates_retriever_from_jd(job_description: str):\n",
    "    # Use structured predict to infer the metadata filters and query string.\n",
    "    metadata_info = await get_metadata(job_description)\n",
    "    filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(key=\"domain\", operator=FilterOperator.EQ, value=metadata_info.domain),\n",
    "        MetadataFilter(key=\"country\", operator=FilterOperator.IN, value=metadata_info.country),\n",
    "        MetadataFilter(key=\"skills\", operator=FilterOperator.IN, value=metadata_info.skills)\n",
    "    ],\n",
    "    condition=FilterCondition.OR\n",
    ")\n",
    "    print(f\"> Inferred filters: {filters.json()}\")\n",
    "    retriever = index.as_retriever(\n",
    "    retrieval_mode=\"chunks\",\n",
    "    metadata_filters=filters,\n",
    "    )\n",
    "    # run query\n",
    "    return retriever.retrieve(job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0LGRz-MXRtl"
   },
   "source": [
    "## Retrieve based on HR query\n",
    "\n",
    "Let's test the process based on a usual sample HR query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MkMluwweMj7O",
    "outputId": "4648f0e3-37f5-47ba-e096-d2e02a4977d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> User query string: I want someone who studied in US, Java developer, and worked in IT\n",
      "> Inferred filters: {\"filters\":[{\"key\":\"domain\",\"value\":\"IT\",\"operator\":\"==\"},{\"key\":\"country\",\"value\":[\"USA\",\"United States\",\"Philippines\",\"China\",\"Netherlands\",\"Sierra Leone\"],\"operator\":\"in\"},{\"key\":\"skills\",\"value\":[\"Java\",\"Troubleshooting\",\"Problem Solving\",\"Communication Skills\",\"Team Collaboration\",\"Project Management\",\"Database Management\",\"Data Analysis\",\"Technical Assistance\",\"IT Management\",\"Cloud computing\",\"Business Intelligence\",\"Systems Architecture\",\"SQL\",\"Microsoft Office\",\"ERP\",\"Business Process Design\",\"Data Warehouse\",\"Project Management\",\"User Relations/User Training\",\"Business Analysis\",\"Disaster recovery\",\"IT Strategy\",\"Networking\",\"Information Security\",\"Technical Trainer\",\"Change Management\",\"Risk Management\",\"Process Improvement\",\"Team Leadership\",\"Client-focused\",\"Results-oriented\",\"Strategic Planning\",\"Budgeting/Cost control\",\"Financial Analysis\",\"Quality Assurance\",\"Sales expertise\",\"Customer Service\",\"Active Listening\",\"Problem Solving\",\"Time Management\",\"Leadership\",\"Training and Development\",\"Documentation\",\"Researching\",\"Analytical reasoning\",\"Excellent communication\",\"Fast Learner\",\"Self-Motivated\",\"Organizational Skills\",\"Team Oriented\",\"Computer proficient\",\"Problem Solver\",\"Billing online system\",\"Customer-oriented\",\"Financial statement analysis\"],\"operator\":\"in\"}],\"condition\":\"or\"}\n"
     ]
    }
   ],
   "source": [
    "query = \"I want someone who studied in US, Java developer, and worked in IT\"\n",
    "nodes = await candidates_retriever_from_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the retrieved candidates resumes file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "idd3UoL4MlYp",
    "outputId": "852844b2-11be-4fca-854a-5e43dd1d6d88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sampled_data/IT/16899268.pdf', 'sampled_data/IT/27536013.pdf']\n"
     ]
    }
   ],
   "source": [
    "print(get_candidates_file_paths(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfBDzbhNXV09"
   },
   "source": [
    "## Retrieve candidate based on JD (Job Description)\n",
    "\n",
    "Here we retrieve candidates based on Job Description.\n",
    "\n",
    "We parse the job description pdf and use it to retrieve the relevant candidates for the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9K3-TQjHXY3i"
   },
   "source": [
    "### Parse Job Description (JD)\n",
    "\n",
    "Here, we parse the sample job_description.pdf that we have downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jVr2ZZHFMmzq",
    "outputId": "e0958b08-faa2-4a24-e0b6-875d1d85c2d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1/1: ./job_description.pdf\n",
      "Started parsing the file under job_id 0df3043a-74dd-40e8-a83b-c93416160d0d\n"
     ]
    }
   ],
   "source": [
    "job_description_file_path = './job_description.pdf'\n",
    "\n",
    "job_description_document = parse_files([job_description_file_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlKmaJY6MoYG"
   },
   "outputs": [],
   "source": [
    "job_description = \"\\n\\n\".join([doc.text for doc in job_description_document[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dogo3GkNMpkd",
    "outputId": "c8a50dc9-c824-4c61-90f5-1dfcafb5d3c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Senior Information Technology Manager\n",
      "\n",
      "# About the Role\n",
      "\n",
      "We are seeking an experienced Information Technology Manager to lead our technology initiatives and drive digital transformation across the organization. The ideal candidate will combine strong technical expertise with business acumen and leadership skills.\n",
      "\n",
      "# Key Responsibilities\n",
      "\n",
      "- Lead and manage a cross-functional IT team in developing and implementing technology solutions\n",
      "- Oversee the planning, implementation, and maintenance of enterprise IT systems and infrastructure\n",
      "- Drive strategic technology initiatives aligned with business objectives\n",
      "- Manage vendor relationships and technology partnerships\n",
      "- Ensure system security, data integrity, and business continuity\n",
      "- Develop and maintain IT policies, procedures, and best practices\n",
      "- Budget planning and resource allocation for IT projects\n",
      "- Provide technical leadership in evaluating and implementing new technologies\n",
      "- Collaborate with stakeholders to identify technology needs and solutions\n",
      "\n",
      "# Required Qualifications\n",
      "\n",
      "- Bachelor's degree in Computer Science, Information Technology, or related field\n",
      "- 8+ years of progressive IT management experience\n",
      "- Strong experience with enterprise systems and infrastructure management\n",
      "- Proven track record of successful project management and delivery\n",
      "- Experience with IT security, compliance, and risk management\n",
      "- Strong knowledge of current technology trends and solutions\n",
      "\n",
      "# Technical Skills\n",
      "\n",
      "- Enterprise Resource Planning (ERP) systems\n",
      "- Network infrastructure and security\n",
      "- Cloud computing platforms and services\n",
      "\n",
      "# IT Service Management Frameworks\n",
      "\n",
      "- Database management systems\n",
      "- System integration and architecture\n",
      "- Virtualization technologies\n",
      "- Disaster recovery and business continuity\n",
      "\n",
      "# Leadership Skills\n",
      "\n",
      "- Team management and development\n",
      "- Strategic planning and execution\n",
      "- Strong communication and presentation skills\n",
      "- Problem-solving and analytical thinking\n",
      "- Change management\n",
      "- Budget management\n",
      "- Stakeholder management\n",
      "- Cross-functional collaboration\n",
      "\n",
      "# Preferred Qualifications\n",
      "\n",
      "- Master's degree in related field\n",
      "- Professional certifications (PMP, ITIL, etc.)\n",
      "- Experience with digital transformation initiatives\n",
      "- Knowledge of agile methodologies\n",
      "- Experience in similar industry\n",
      "\n",
      "# What We Offer\n",
      "\n",
      "- Competitive salary and benefits package\n",
      "- Professional development opportunities\n",
      "- Collaborative work environment\n",
      "- Opportunity to drive technological innovation\n",
      "- Work-life balance\n",
      "- Career advancement potential\n",
      "\n",
      "The ideal candidate will be a results-driven leader who can balance technical expertise with business strategy, while effectively managing teams and stakeholders across the organization.\n"
     ]
    }
   ],
   "source": [
    "print(job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaQq0v4xXdHg"
   },
   "source": [
    "### Retrieve candidates\n",
    "\n",
    "Here we retrieve candidates based on the job description text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eR1ZHslDMtZJ",
    "outputId": "f9f1aafe-247e-4b35-a346-a6992e28b2d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Inferred filters: {\"filters\":[{\"key\":\"domain\",\"value\":\"IT\",\"operator\":\"==\"},{\"key\":\"country\",\"value\":[\"United States\"],\"operator\":\"in\"},{\"key\":\"skills\",\"value\":[\"Enterprise Resource Planning (ERP) systems\",\"Network infrastructure and security\",\"Cloud computing platforms and services\",\"Database management systems\",\"System integration and architecture\",\"Virtualization technologies\",\"Disaster recovery and business continuity\",\"Team management and development\",\"Strategic planning and execution\",\"Strong communication and presentation skills\",\"Problem-solving and analytical thinking\",\"Change management\",\"Budget management\",\"Stakeholder management\",\"Cross-functional collaboration\"],\"operator\":\"in\"}],\"condition\":\"or\"}\n"
     ]
    }
   ],
   "source": [
    "candidates_based_on_jd = await candidates_retriever_from_jd(job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_hdQbXCMvSw"
   },
   "outputs": [],
   "source": [
    "candidates_file_paths = get_candidates_file_paths(candidates_based_on_jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r5DFzje1P1-C",
    "outputId": "8df11b4b-0cc7-46be-fdb0-a21020c6b898"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sampled_data/IT/18159866.pdf', 'sampled_data/FINANCE/25101183.pdf', 'sampled_data/IT/27536013.pdf']\n"
     ]
    }
   ],
   "source": [
    "print(candidates_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBJaAwAlXiKL"
   },
   "source": [
    "## Analyse candidate resume based on retrieval\n",
    "\n",
    "Once we have the relevant candidate resumes, we need to analyze why, how, and which candidates are suitable for the job description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7YmWH3uXm7p"
   },
   "source": [
    "### Parse the candidate resumes\n",
    "\n",
    "Here, we parse the candidate resumes retrieved based on the job description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3LxRhGjQP4ml",
    "outputId": "f200ba2f-5a34-4f9b-9d9d-d661d0cb5fea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1/3: sampled_data/IT/18159866.pdf\n",
      "Started parsing the file under job_id 50c1cf22-b692-4521-b40c-62f6a31e1215\n",
      "Processing file 2/3: sampled_data/FINANCE/25101183.pdf\n",
      "Started parsing the file under job_id 85992196-4c31-474c-8423-3ead6fe5835f\n",
      "Processing file 3/3: sampled_data/IT/27536013.pdf\n",
      "Started parsing the file under job_id e4263df3-4611-453c-834d-3c0eecf522be\n"
     ]
    }
   ],
   "source": [
    "candidates_resumes = parse_files(candidates_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YAcJ2b0P9g1"
   },
   "outputs": [],
   "source": [
    "candidates_resumes_text = \"\\n\\n\".join([doc.text for docs in candidates_resumes for doc in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BX27yyvXpeP"
   },
   "source": [
    "### Analyses\n",
    "\n",
    "Let's analyze the candidate resumes against the job description by processing them through the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "of6qb4S_Qb4J"
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"Based on the following job description, please share the analysis of why specific candidates are suitable for the job.\n",
    "\n",
    "        Job Description:\n",
    "        {job_description}\n",
    "\n",
    "        Candidates:\n",
    "        {candidates_resumes_text}\n",
    "        \"\"\"\n",
    "\n",
    "analyses = llm.complete(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyGtOSpSQx_x",
    "outputId": "d73e4559-2db4-404e-958c-978064977bdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the job description for the Senior Information Technology Manager position and the profiles of the candidates provided, here is an analysis of why specific candidates may be suitable for the job:\n",
      "\n",
      "### Candidate 1: Senior Vice President of Global Information Technology\n",
      "\n",
      "**Strengths:**\n",
      "1. **Extensive Experience:** With over 20 years in IT management, including a current role as Senior Vice President, this candidate has significant experience leading large teams and managing complex IT environments, which aligns well with the requirement for 8+ years of progressive IT management experience.\n",
      "   \n",
      "2. **Project Management Expertise:** The candidate has a proven track record of managing cross-functional teams on large implementations and development projects, which is crucial for overseeing the planning, implementation, and maintenance of enterprise IT systems.\n",
      "\n",
      "3. **Strategic Planning and Execution:** Their experience in strategic planning and change implementation demonstrates the ability to drive strategic technology initiatives aligned with business objectives.\n",
      "\n",
      "4. **Risk Management and Security:** The candidate has conducted periodic risk assessments and developed disaster recovery plans, which is essential for ensuring system security, data integrity, and business continuity.\n",
      "\n",
      "5. **Cost Savings and Efficiency:** The candidate has a history of delivering projects on time and within budget, realizing significant improvements in processing efficiency, which is a key responsibility in the job description.\n",
      "\n",
      "**Suitability:** This candidate is highly suitable due to their extensive leadership experience, strategic planning capabilities, and proven success in managing large-scale IT projects and teams.\n",
      "\n",
      "---\n",
      "\n",
      "### Candidate 2: Program Manager / PMO Director\n",
      "\n",
      "**Strengths:**\n",
      "1. **Program Management Experience:** This candidate has a strong background in program management and has led multi-functional technology teams, which is essential for managing a cross-functional IT team.\n",
      "\n",
      "2. **Financial Acumen:** Their experience in managing budgets, financial analysis, and cost savings initiatives aligns well with the job's requirement for budget planning and resource allocation for IT projects.\n",
      "\n",
      "3. **Change Management:** The candidate has a proven ability to turn around underperforming programs and lead successful change initiatives, which is important for driving digital transformation across the organization.\n",
      "\n",
      "4. **Stakeholder Management:** They have demonstrated skills in stakeholder management and collaboration, which are critical for identifying technology needs and solutions in collaboration with various stakeholders.\n",
      "\n",
      "5. **Certifications:** The candidate holds relevant certifications such as PMP and ITIL, which are preferred qualifications for the role.\n",
      "\n",
      "**Suitability:** This candidate is suitable due to their strong program management skills, financial expertise, and experience in leading change initiatives, making them a good fit for the strategic and operational aspects of the role.\n",
      "\n",
      "---\n",
      "\n",
      "### Candidate 3: Experienced Information Technology Manager\n",
      "\n",
      "**Strengths:**\n",
      "1. **Diverse IT Management Experience:** With over 10 years of experience in various management areas, this candidate has a solid foundation in IT management, which meets the job's experience requirement.\n",
      "\n",
      "2. **Project Management Skills:** The candidate has demonstrated proficiency in project management, managing resources, and personnel, which is essential for leading and managing an IT team.\n",
      "\n",
      "3. **Business Intelligence and Reporting:** Their experience in redesigning BI programs and implementing effective systems indicates a strong understanding of technology solutions that can drive business objectives.\n",
      "\n",
      "4. **User Relations and Training:** The candidate has experience in user relations and training, which is important for collaborating with stakeholders to identify technology needs and solutions.\n",
      "\n",
      "5. **Cost Savings Initiatives:** They have successfully implemented projects that resulted in significant cost savings, aligning with the job's focus on budget management and resource allocation.\n",
      "\n",
      "**Suitability:** This candidate is suitable due to their solid IT management experience, project management skills, and ability to implement effective technology solutions that align with business needs.\n",
      "\n",
      "---\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Each candidate brings a unique set of skills and experiences that align with the requirements of the Senior Information Technology Manager position. Candidate 1 stands out for their extensive leadership experience and strategic planning capabilities. Candidate 2 offers strong program management and financial acumen, while Candidate 3 provides a solid foundation in IT management and project execution. Depending on the specific needs and culture of the organization, any of these candidates could be a strong fit for the role.\n"
     ]
    }
   ],
   "source": [
    "print(analyses)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CareerConnect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
